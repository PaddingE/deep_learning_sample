{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 와인 예측\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2    3      4     5     6       7     8     9    10  11  12\n",
       "0   7.4  0.70  0.00  1.9  0.076  11.0  34.0  0.9978  3.51  0.56  9.4   5   1\n",
       "1   7.8  0.88  0.00  2.6  0.098  25.0  67.0  0.9968  3.20  0.68  9.8   5   1\n",
       "2   7.8  0.76  0.04  2.3  0.092  15.0  54.0  0.9970  3.26  0.65  9.8   5   1\n",
       "3  11.2  0.28  0.56  1.9  0.075  17.0  60.0  0.9980  3.16  0.58  9.8   6   1\n",
       "4   7.4  0.70  0.00  1.9  0.076  11.0  34.0  0.9978  3.51  0.56  9.4   5   1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/wine.csv', header= None)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,0:-1]\n",
    "y = df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size= 0.2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 30)                390       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 248       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 647\n",
      "Trainable params: 647\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim = X_train.shape[1], activation = 'relu'))\n",
    "model.add(Dense(8, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0708 - accuracy: 0.9779 - val_loss: 0.0915 - val_accuracy: 0.9715\n",
      "Epoch 2/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0597 - accuracy: 0.9813 - val_loss: 0.0876 - val_accuracy: 0.9738\n",
      "Epoch 3/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0594 - accuracy: 0.9802 - val_loss: 0.0852 - val_accuracy: 0.9746\n",
      "Epoch 4/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0577 - accuracy: 0.9815 - val_loss: 0.0915 - val_accuracy: 0.9715\n",
      "Epoch 5/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0560 - accuracy: 0.9820 - val_loss: 0.1204 - val_accuracy: 0.9692\n",
      "Epoch 6/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0580 - accuracy: 0.9823 - val_loss: 0.0981 - val_accuracy: 0.9738\n",
      "Epoch 7/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0611 - accuracy: 0.9810 - val_loss: 0.0969 - val_accuracy: 0.9746\n",
      "Epoch 8/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0648 - accuracy: 0.9764 - val_loss: 0.0861 - val_accuracy: 0.9738\n",
      "Epoch 9/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0591 - accuracy: 0.9810 - val_loss: 0.0935 - val_accuracy: 0.9754\n",
      "Epoch 10/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0534 - accuracy: 0.9843 - val_loss: 0.0844 - val_accuracy: 0.9762\n",
      "Epoch 11/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0587 - accuracy: 0.9838 - val_loss: 0.0899 - val_accuracy: 0.9746\n",
      "Epoch 12/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0539 - accuracy: 0.9843 - val_loss: 0.0943 - val_accuracy: 0.9746\n",
      "Epoch 13/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0551 - accuracy: 0.9831 - val_loss: 0.1112 - val_accuracy: 0.9646\n",
      "Epoch 14/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0603 - accuracy: 0.9823 - val_loss: 0.0910 - val_accuracy: 0.9754\n",
      "Epoch 15/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0571 - accuracy: 0.9815 - val_loss: 0.0977 - val_accuracy: 0.9754\n",
      "Epoch 16/50\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0525 - accuracy: 0.9838 - val_loss: 0.0853 - val_accuracy: 0.9738\n",
      "Epoch 17/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0600 - accuracy: 0.9820 - val_loss: 0.0881 - val_accuracy: 0.9754\n",
      "Epoch 18/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0561 - accuracy: 0.9828 - val_loss: 0.1009 - val_accuracy: 0.9746\n",
      "Epoch 19/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0553 - accuracy: 0.9826 - val_loss: 0.0917 - val_accuracy: 0.9715\n",
      "Epoch 20/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0548 - accuracy: 0.9818 - val_loss: 0.0971 - val_accuracy: 0.9762\n",
      "Epoch 21/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0572 - accuracy: 0.9828 - val_loss: 0.0911 - val_accuracy: 0.9754\n",
      "Epoch 22/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0534 - accuracy: 0.9836 - val_loss: 0.0799 - val_accuracy: 0.9754\n",
      "Epoch 23/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0563 - accuracy: 0.9826 - val_loss: 0.0814 - val_accuracy: 0.9754\n",
      "Epoch 24/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0535 - accuracy: 0.9836 - val_loss: 0.0971 - val_accuracy: 0.9754\n",
      "Epoch 25/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0515 - accuracy: 0.9838 - val_loss: 0.0873 - val_accuracy: 0.9762\n",
      "Epoch 26/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0499 - accuracy: 0.9849 - val_loss: 0.0902 - val_accuracy: 0.9754\n",
      "Epoch 27/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0544 - accuracy: 0.9815 - val_loss: 0.0854 - val_accuracy: 0.9762\n",
      "Epoch 28/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0636 - accuracy: 0.9805 - val_loss: 0.0878 - val_accuracy: 0.9746\n",
      "Epoch 29/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0627 - accuracy: 0.9792 - val_loss: 0.0900 - val_accuracy: 0.9738\n",
      "Epoch 30/50\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0535 - accuracy: 0.9833 - val_loss: 0.0808 - val_accuracy: 0.9777\n",
      "Epoch 31/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0518 - accuracy: 0.9854 - val_loss: 0.0987 - val_accuracy: 0.9769\n",
      "Epoch 32/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0584 - accuracy: 0.9828 - val_loss: 0.1623 - val_accuracy: 0.9654\n",
      "Epoch 33/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0500 - accuracy: 0.9833 - val_loss: 0.0823 - val_accuracy: 0.9777\n",
      "Epoch 34/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0548 - accuracy: 0.9823 - val_loss: 0.1083 - val_accuracy: 0.9754\n",
      "Epoch 35/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0526 - accuracy: 0.9849 - val_loss: 0.0873 - val_accuracy: 0.9754\n",
      "Epoch 36/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0501 - accuracy: 0.9841 - val_loss: 0.0870 - val_accuracy: 0.9762\n",
      "Epoch 37/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0497 - accuracy: 0.9874 - val_loss: 0.0843 - val_accuracy: 0.9762\n",
      "Epoch 38/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0601 - accuracy: 0.9808 - val_loss: 0.0880 - val_accuracy: 0.9762\n",
      "Epoch 39/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0505 - accuracy: 0.9838 - val_loss: 0.1018 - val_accuracy: 0.9669\n",
      "Epoch 40/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0582 - accuracy: 0.9813 - val_loss: 0.0787 - val_accuracy: 0.9777\n",
      "Epoch 41/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0526 - accuracy: 0.9846 - val_loss: 0.0801 - val_accuracy: 0.9769\n",
      "Epoch 42/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0507 - accuracy: 0.9833 - val_loss: 0.0867 - val_accuracy: 0.9769\n",
      "Epoch 43/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0474 - accuracy: 0.9851 - val_loss: 0.0898 - val_accuracy: 0.9754\n",
      "Epoch 44/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0505 - accuracy: 0.9841 - val_loss: 0.0874 - val_accuracy: 0.9762\n",
      "Epoch 45/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0547 - accuracy: 0.9838 - val_loss: 0.0820 - val_accuracy: 0.9754\n",
      "Epoch 46/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0495 - accuracy: 0.9856 - val_loss: 0.0879 - val_accuracy: 0.9746\n",
      "Epoch 47/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0539 - accuracy: 0.9836 - val_loss: 0.0799 - val_accuracy: 0.9792\n",
      "Epoch 48/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0499 - accuracy: 0.9838 - val_loss: 0.1042 - val_accuracy: 0.9662\n",
      "Epoch 49/50\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0517 - accuracy: 0.9851 - val_loss: 0.0838 - val_accuracy: 0.9762\n",
      "Epoch 50/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0517 - accuracy: 0.9849 - val_loss: 0.0823 - val_accuracy: 0.9738\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss= 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "hist = model.fit(X_train, y_train, epochs=50, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.07078945636749268,\n",
       "  0.059716761112213135,\n",
       "  0.059364888817071915,\n",
       "  0.057696931064128876,\n",
       "  0.055971018970012665,\n",
       "  0.05795975401997566,\n",
       "  0.06110548973083496,\n",
       "  0.06484188884496689,\n",
       "  0.0590943768620491,\n",
       "  0.053407009690999985,\n",
       "  0.058659132570028305,\n",
       "  0.05393969640135765,\n",
       "  0.055064212530851364,\n",
       "  0.060282785445451736,\n",
       "  0.05714401602745056,\n",
       "  0.052519265562295914,\n",
       "  0.06001732870936394,\n",
       "  0.0560561828315258,\n",
       "  0.05530313774943352,\n",
       "  0.05484255030751228,\n",
       "  0.057181477546691895,\n",
       "  0.05344230681657791,\n",
       "  0.05627159774303436,\n",
       "  0.05350835248827934,\n",
       "  0.05149844288825989,\n",
       "  0.04990534856915474,\n",
       "  0.05440548062324524,\n",
       "  0.06363353133201599,\n",
       "  0.06271743029356003,\n",
       "  0.053535476326942444,\n",
       "  0.05175650492310524,\n",
       "  0.05836411193013191,\n",
       "  0.05002550035715103,\n",
       "  0.05484499782323837,\n",
       "  0.052602868527173996,\n",
       "  0.05010026693344116,\n",
       "  0.04972592368721962,\n",
       "  0.060062751173973083,\n",
       "  0.05054822564125061,\n",
       "  0.0581568144261837,\n",
       "  0.052635401487350464,\n",
       "  0.050681497901678085,\n",
       "  0.04739111661911011,\n",
       "  0.050505537539720535,\n",
       "  0.05472450703382492,\n",
       "  0.04954414442181587,\n",
       "  0.05390835553407669,\n",
       "  0.04992436617612839,\n",
       "  0.05166265740990639,\n",
       "  0.051710788160562515],\n",
       " 'accuracy': [0.9779317378997803,\n",
       "  0.9812676310539246,\n",
       "  0.9802412390708923,\n",
       "  0.9815242290496826,\n",
       "  0.9820374846458435,\n",
       "  0.9822940826416016,\n",
       "  0.9810110330581665,\n",
       "  0.9763920903205872,\n",
       "  0.9810110330581665,\n",
       "  0.9843469262123108,\n",
       "  0.9838337302207947,\n",
       "  0.9843469262123108,\n",
       "  0.9830638766288757,\n",
       "  0.9822940826416016,\n",
       "  0.9815242290496826,\n",
       "  0.9838337302207947,\n",
       "  0.9820374846458435,\n",
       "  0.9828072786331177,\n",
       "  0.9825506806373596,\n",
       "  0.9817808866500854,\n",
       "  0.9828072786331177,\n",
       "  0.9835771322250366,\n",
       "  0.9825506806373596,\n",
       "  0.9835771322250366,\n",
       "  0.9838337302207947,\n",
       "  0.9848601222038269,\n",
       "  0.9815242290496826,\n",
       "  0.9804978370666504,\n",
       "  0.9792147874832153,\n",
       "  0.9833204746246338,\n",
       "  0.9853733777999878,\n",
       "  0.9828072786331177,\n",
       "  0.9833204746246338,\n",
       "  0.9822940826416016,\n",
       "  0.9848601222038269,\n",
       "  0.9840903282165527,\n",
       "  0.987426221370697,\n",
       "  0.9807544350624084,\n",
       "  0.9838337302207947,\n",
       "  0.9812676310539246,\n",
       "  0.9846035242080688,\n",
       "  0.9833204746246338,\n",
       "  0.9851167798042297,\n",
       "  0.9840903282165527,\n",
       "  0.9838337302207947,\n",
       "  0.9856299757957458,\n",
       "  0.9835771322250366,\n",
       "  0.9838337302207947,\n",
       "  0.9851167798042297,\n",
       "  0.9848601222038269],\n",
       " 'val_loss': [0.09147673845291138,\n",
       "  0.08760672807693481,\n",
       "  0.08515862375497818,\n",
       "  0.09150073677301407,\n",
       "  0.12042683362960815,\n",
       "  0.09810156375169754,\n",
       "  0.09686083346605301,\n",
       "  0.08608630299568176,\n",
       "  0.09349803626537323,\n",
       "  0.08437970280647278,\n",
       "  0.08990807086229324,\n",
       "  0.09431882202625275,\n",
       "  0.11116739362478256,\n",
       "  0.09097509831190109,\n",
       "  0.09766766428947449,\n",
       "  0.08531228452920914,\n",
       "  0.08813738822937012,\n",
       "  0.10086985677480698,\n",
       "  0.09172659367322922,\n",
       "  0.09708337485790253,\n",
       "  0.09111953526735306,\n",
       "  0.07991352677345276,\n",
       "  0.0814233049750328,\n",
       "  0.09706588089466095,\n",
       "  0.08731625229120255,\n",
       "  0.09019815921783447,\n",
       "  0.08539042621850967,\n",
       "  0.08780978620052338,\n",
       "  0.09003432840108871,\n",
       "  0.08079618960618973,\n",
       "  0.09868558496236801,\n",
       "  0.1622873693704605,\n",
       "  0.08231955021619797,\n",
       "  0.10831543058156967,\n",
       "  0.08733026683330536,\n",
       "  0.08700250089168549,\n",
       "  0.08425232768058777,\n",
       "  0.08802423626184464,\n",
       "  0.10175909101963043,\n",
       "  0.07874476909637451,\n",
       "  0.08006293326616287,\n",
       "  0.08668224513530731,\n",
       "  0.0898226872086525,\n",
       "  0.08736462891101837,\n",
       "  0.08204208314418793,\n",
       "  0.08789929747581482,\n",
       "  0.07994705438613892,\n",
       "  0.10420005023479462,\n",
       "  0.08381226658821106,\n",
       "  0.08226119726896286],\n",
       " 'val_accuracy': [0.9715384840965271,\n",
       "  0.9738461375236511,\n",
       "  0.9746153950691223,\n",
       "  0.9715384840965271,\n",
       "  0.9692307710647583,\n",
       "  0.9738461375236511,\n",
       "  0.9746153950691223,\n",
       "  0.9738461375236511,\n",
       "  0.9753845930099487,\n",
       "  0.9761538505554199,\n",
       "  0.9746153950691223,\n",
       "  0.9746153950691223,\n",
       "  0.9646154046058655,\n",
       "  0.9753845930099487,\n",
       "  0.9753845930099487,\n",
       "  0.9738461375236511,\n",
       "  0.9753845930099487,\n",
       "  0.9746153950691223,\n",
       "  0.9715384840965271,\n",
       "  0.9761538505554199,\n",
       "  0.9753845930099487,\n",
       "  0.9753845930099487,\n",
       "  0.9753845930099487,\n",
       "  0.9753845930099487,\n",
       "  0.9761538505554199,\n",
       "  0.9753845930099487,\n",
       "  0.9761538505554199,\n",
       "  0.9746153950691223,\n",
       "  0.9738461375236511,\n",
       "  0.9776923060417175,\n",
       "  0.9769230484962463,\n",
       "  0.9653846025466919,\n",
       "  0.9776923060417175,\n",
       "  0.9753845930099487,\n",
       "  0.9753845930099487,\n",
       "  0.9761538505554199,\n",
       "  0.9761538505554199,\n",
       "  0.9761538505554199,\n",
       "  0.9669230580329895,\n",
       "  0.9776923060417175,\n",
       "  0.9769230484962463,\n",
       "  0.9769230484962463,\n",
       "  0.9753845930099487,\n",
       "  0.9761538505554199,\n",
       "  0.9753845930099487,\n",
       "  0.9746153950691223,\n",
       "  0.9792307615280151,\n",
       "  0.9661538600921631,\n",
       "  0.9761538505554199,\n",
       "  0.9738461375236511]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모델 업데이트 하면서 모델 학습 실행\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './data/model/{epoch:02d} - {val_accuracy:4f}.hdf5'\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=filename, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to ./data/model\\01 - 0.976154.hdf5\n",
      "\n",
      "Epoch 2: saving model to ./data/model\\02 - 0.978462.hdf5\n",
      "\n",
      "Epoch 3: saving model to ./data/model\\03 - 0.979231.hdf5\n",
      "\n",
      "Epoch 4: saving model to ./data/model\\04 - 0.970000.hdf5\n",
      "\n",
      "Epoch 5: saving model to ./data/model\\05 - 0.976923.hdf5\n",
      "\n",
      "Epoch 6: saving model to ./data/model\\06 - 0.976154.hdf5\n",
      "\n",
      "Epoch 7: saving model to ./data/model\\07 - 0.975385.hdf5\n",
      "\n",
      "Epoch 8: saving model to ./data/model\\08 - 0.976923.hdf5\n",
      "\n",
      "Epoch 9: saving model to ./data/model\\09 - 0.976154.hdf5\n",
      "\n",
      "Epoch 10: saving model to ./data/model\\10 - 0.976154.hdf5\n",
      "\n",
      "Epoch 11: saving model to ./data/model\\11 - 0.976154.hdf5\n",
      "\n",
      "Epoch 12: saving model to ./data/model\\12 - 0.974615.hdf5\n",
      "\n",
      "Epoch 13: saving model to ./data/model\\13 - 0.978462.hdf5\n",
      "\n",
      "Epoch 14: saving model to ./data/model\\14 - 0.976154.hdf5\n",
      "\n",
      "Epoch 15: saving model to ./data/model\\15 - 0.977692.hdf5\n",
      "\n",
      "Epoch 16: saving model to ./data/model\\16 - 0.978462.hdf5\n",
      "\n",
      "Epoch 17: saving model to ./data/model\\17 - 0.979231.hdf5\n",
      "\n",
      "Epoch 18: saving model to ./data/model\\18 - 0.962308.hdf5\n",
      "\n",
      "Epoch 19: saving model to ./data/model\\19 - 0.980769.hdf5\n",
      "\n",
      "Epoch 20: saving model to ./data/model\\20 - 0.978462.hdf5\n",
      "\n",
      "Epoch 21: saving model to ./data/model\\21 - 0.976154.hdf5\n",
      "\n",
      "Epoch 22: saving model to ./data/model\\22 - 0.978462.hdf5\n",
      "\n",
      "Epoch 23: saving model to ./data/model\\23 - 0.974615.hdf5\n",
      "\n",
      "Epoch 24: saving model to ./data/model\\24 - 0.980000.hdf5\n",
      "\n",
      "Epoch 25: saving model to ./data/model\\25 - 0.973846.hdf5\n",
      "\n",
      "Epoch 26: saving model to ./data/model\\26 - 0.975385.hdf5\n",
      "\n",
      "Epoch 27: saving model to ./data/model\\27 - 0.965385.hdf5\n",
      "\n",
      "Epoch 28: saving model to ./data/model\\28 - 0.975385.hdf5\n",
      "\n",
      "Epoch 29: saving model to ./data/model\\29 - 0.976154.hdf5\n",
      "\n",
      "Epoch 30: saving model to ./data/model\\30 - 0.976923.hdf5\n",
      "\n",
      "Epoch 31: saving model to ./data/model\\31 - 0.978462.hdf5\n",
      "\n",
      "Epoch 32: saving model to ./data/model\\32 - 0.979231.hdf5\n",
      "\n",
      "Epoch 33: saving model to ./data/model\\33 - 0.976923.hdf5\n",
      "\n",
      "Epoch 34: saving model to ./data/model\\34 - 0.975385.hdf5\n",
      "\n",
      "Epoch 35: saving model to ./data/model\\35 - 0.976154.hdf5\n",
      "\n",
      "Epoch 36: saving model to ./data/model\\36 - 0.974615.hdf5\n",
      "\n",
      "Epoch 37: saving model to ./data/model\\37 - 0.978462.hdf5\n",
      "\n",
      "Epoch 38: saving model to ./data/model\\38 - 0.979231.hdf5\n",
      "\n",
      "Epoch 39: saving model to ./data/model\\39 - 0.976154.hdf5\n",
      "\n",
      "Epoch 40: saving model to ./data/model\\40 - 0.963077.hdf5\n",
      "\n",
      "Epoch 41: saving model to ./data/model\\41 - 0.976923.hdf5\n",
      "\n",
      "Epoch 42: saving model to ./data/model\\42 - 0.976923.hdf5\n",
      "\n",
      "Epoch 43: saving model to ./data/model\\43 - 0.970000.hdf5\n",
      "\n",
      "Epoch 44: saving model to ./data/model\\44 - 0.976154.hdf5\n",
      "\n",
      "Epoch 45: saving model to ./data/model\\45 - 0.976923.hdf5\n",
      "\n",
      "Epoch 46: saving model to ./data/model\\46 - 0.978462.hdf5\n",
      "\n",
      "Epoch 47: saving model to ./data/model\\47 - 0.980000.hdf5\n",
      "\n",
      "Epoch 48: saving model to ./data/model\\48 - 0.976923.hdf5\n",
      "\n",
      "Epoch 49: saving model to ./data/model\\49 - 0.977692.hdf5\n",
      "\n",
      "Epoch 50: saving model to ./data/model\\50 - 0.975385.hdf5\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, y_train, epochs=50, validation_split=0.25, verbose=0, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0633 - accuracy: 0.9777\n",
      "test accuracy:  0.9776923060417175\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test,y_test)\n",
    "print('test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 학습을 2000회 실행\n",
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim = X_train.shape[1], activation = 'relu'))\n",
    "model.add(Dense(8, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss= 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#hist = model.fit(X_train, y_train, epochs=2000, validation_split=0.25, verbose=0, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "hist_df = pd.DataFrame(hist.history)\n",
    "hist_df.head()\n",
    "\n",
    "y_loss = hist_df['loss']\n",
    "y_vloss = hist_df['val_loss']\n",
    "x_len = np.arange(len(y_loss))\n",
    "\n",
    "plt.plot(x_len, y_loss, 'o', c= 'blue', label = 'Train_loss')\n",
    "plt.plot(x_len, y_vloss, 'o', c= 'red', label = 'val_loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 중단 함수를 이용해서 최상의 모델을 찾음\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss', patience = 10)\n",
    "\n",
    "model_name = './data/model/wine_bestmodel.hdf5'\n",
    "checkpointer = ModelCheckpoint(filepath = model_name, monitor= 'val_loss', verbose=0,save_best_only=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "8/8 [==============================] - 1s 51ms/step - loss: 7.5751 - accuracy: 0.2325 - val_loss: 2.5845 - val_accuracy: 0.2377\n",
      "Epoch 2/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1.2073 - accuracy: 0.5666 - val_loss: 0.7820 - val_accuracy: 0.7869\n",
      "Epoch 3/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.9796 - accuracy: 0.7829 - val_loss: 0.8680 - val_accuracy: 0.8169\n",
      "Epoch 4/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.8765 - accuracy: 0.8265 - val_loss: 0.5461 - val_accuracy: 0.8777\n",
      "Epoch 5/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.4407 - accuracy: 0.8907 - val_loss: 0.2424 - val_accuracy: 0.8954\n",
      "Epoch 6/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.3236 - accuracy: 0.8560 - val_loss: 0.2242 - val_accuracy: 0.9054\n",
      "Epoch 7/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2614 - accuracy: 0.9181 - val_loss: 0.2273 - val_accuracy: 0.9362\n",
      "Epoch 8/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2549 - accuracy: 0.9207 - val_loss: 0.2331 - val_accuracy: 0.8969\n",
      "Epoch 9/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2476 - accuracy: 0.9133 - val_loss: 0.2147 - val_accuracy: 0.9323\n",
      "Epoch 10/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.2417 - accuracy: 0.9256 - val_loss: 0.2100 - val_accuracy: 0.9246\n",
      "Epoch 11/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.2339 - accuracy: 0.9148 - val_loss: 0.2019 - val_accuracy: 0.9285\n",
      "Epoch 12/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.2242 - accuracy: 0.9243 - val_loss: 0.1930 - val_accuracy: 0.9346\n",
      "Epoch 13/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.2154 - accuracy: 0.9266 - val_loss: 0.1892 - val_accuracy: 0.9354\n",
      "Epoch 14/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.2123 - accuracy: 0.9264 - val_loss: 0.1850 - val_accuracy: 0.9362\n",
      "Epoch 15/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2108 - accuracy: 0.9271 - val_loss: 0.1832 - val_accuracy: 0.9354\n",
      "Epoch 16/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.2077 - accuracy: 0.9284 - val_loss: 0.1815 - val_accuracy: 0.9346\n",
      "Epoch 17/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.2050 - accuracy: 0.9297 - val_loss: 0.1790 - val_accuracy: 0.9362\n",
      "Epoch 18/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.2026 - accuracy: 0.9305 - val_loss: 0.1779 - val_accuracy: 0.9385\n",
      "Epoch 19/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.2004 - accuracy: 0.9315 - val_loss: 0.1750 - val_accuracy: 0.9369\n",
      "Epoch 20/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1979 - accuracy: 0.9312 - val_loss: 0.1743 - val_accuracy: 0.9392\n",
      "Epoch 21/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1958 - accuracy: 0.9320 - val_loss: 0.1715 - val_accuracy: 0.9415\n",
      "Epoch 22/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1937 - accuracy: 0.9325 - val_loss: 0.1701 - val_accuracy: 0.9415\n",
      "Epoch 23/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1918 - accuracy: 0.9328 - val_loss: 0.1687 - val_accuracy: 0.9415\n",
      "Epoch 24/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1896 - accuracy: 0.9333 - val_loss: 0.1663 - val_accuracy: 0.9415\n",
      "Epoch 25/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1881 - accuracy: 0.9343 - val_loss: 0.1654 - val_accuracy: 0.9438\n",
      "Epoch 26/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1865 - accuracy: 0.9343 - val_loss: 0.1628 - val_accuracy: 0.9438\n",
      "Epoch 27/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1864 - accuracy: 0.9356 - val_loss: 0.1612 - val_accuracy: 0.9415\n",
      "Epoch 28/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1854 - accuracy: 0.9335 - val_loss: 0.1608 - val_accuracy: 0.9469\n",
      "Epoch 29/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1813 - accuracy: 0.9356 - val_loss: 0.1581 - val_accuracy: 0.9462\n",
      "Epoch 30/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1792 - accuracy: 0.9358 - val_loss: 0.1566 - val_accuracy: 0.9462\n",
      "Epoch 31/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1777 - accuracy: 0.9374 - val_loss: 0.1543 - val_accuracy: 0.9462\n",
      "Epoch 32/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1764 - accuracy: 0.9376 - val_loss: 0.1525 - val_accuracy: 0.9454\n",
      "Epoch 33/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1751 - accuracy: 0.9374 - val_loss: 0.1528 - val_accuracy: 0.9508\n",
      "Epoch 34/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1733 - accuracy: 0.9389 - val_loss: 0.1497 - val_accuracy: 0.9477\n",
      "Epoch 35/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1712 - accuracy: 0.9379 - val_loss: 0.1523 - val_accuracy: 0.9538\n",
      "Epoch 36/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1721 - accuracy: 0.9407 - val_loss: 0.1466 - val_accuracy: 0.9454\n",
      "Epoch 37/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1688 - accuracy: 0.9389 - val_loss: 0.1469 - val_accuracy: 0.9515\n",
      "Epoch 38/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1682 - accuracy: 0.9397 - val_loss: 0.1439 - val_accuracy: 0.9492\n",
      "Epoch 39/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1662 - accuracy: 0.9425 - val_loss: 0.1421 - val_accuracy: 0.9485\n",
      "Epoch 40/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1648 - accuracy: 0.9402 - val_loss: 0.1444 - val_accuracy: 0.9546\n",
      "Epoch 41/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1640 - accuracy: 0.9441 - val_loss: 0.1405 - val_accuracy: 0.9469\n",
      "Epoch 42/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1630 - accuracy: 0.9425 - val_loss: 0.1394 - val_accuracy: 0.9538\n",
      "Epoch 43/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1631 - accuracy: 0.9423 - val_loss: 0.1413 - val_accuracy: 0.9523\n",
      "Epoch 44/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1614 - accuracy: 0.9438 - val_loss: 0.1363 - val_accuracy: 0.9485\n",
      "Epoch 45/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1593 - accuracy: 0.9441 - val_loss: 0.1369 - val_accuracy: 0.9546\n",
      "Epoch 46/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1571 - accuracy: 0.9441 - val_loss: 0.1334 - val_accuracy: 0.9554\n",
      "Epoch 47/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1552 - accuracy: 0.9456 - val_loss: 0.1319 - val_accuracy: 0.9546\n",
      "Epoch 48/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1540 - accuracy: 0.9446 - val_loss: 0.1317 - val_accuracy: 0.9562\n",
      "Epoch 49/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1535 - accuracy: 0.9474 - val_loss: 0.1300 - val_accuracy: 0.9546\n",
      "Epoch 50/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1543 - accuracy: 0.9464 - val_loss: 0.1282 - val_accuracy: 0.9554\n",
      "Epoch 51/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1518 - accuracy: 0.9484 - val_loss: 0.1273 - val_accuracy: 0.9577\n",
      "Epoch 52/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1501 - accuracy: 0.9474 - val_loss: 0.1287 - val_accuracy: 0.9569\n",
      "Epoch 53/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1500 - accuracy: 0.9489 - val_loss: 0.1253 - val_accuracy: 0.9577\n",
      "Epoch 54/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.1486 - accuracy: 0.9487 - val_loss: 0.1240 - val_accuracy: 0.9585\n",
      "Epoch 55/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1480 - accuracy: 0.9474 - val_loss: 0.1248 - val_accuracy: 0.9569\n",
      "Epoch 56/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1456 - accuracy: 0.9500 - val_loss: 0.1215 - val_accuracy: 0.9585\n",
      "Epoch 57/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1448 - accuracy: 0.9497 - val_loss: 0.1206 - val_accuracy: 0.9585\n",
      "Epoch 58/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1434 - accuracy: 0.9494 - val_loss: 0.1205 - val_accuracy: 0.9577\n",
      "Epoch 59/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1423 - accuracy: 0.9497 - val_loss: 0.1188 - val_accuracy: 0.9592\n",
      "Epoch 60/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1419 - accuracy: 0.9502 - val_loss: 0.1177 - val_accuracy: 0.9600\n",
      "Epoch 61/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1410 - accuracy: 0.9500 - val_loss: 0.1197 - val_accuracy: 0.9585\n",
      "Epoch 62/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1412 - accuracy: 0.9502 - val_loss: 0.1163 - val_accuracy: 0.9592\n",
      "Epoch 63/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1398 - accuracy: 0.9528 - val_loss: 0.1146 - val_accuracy: 0.9608\n",
      "Epoch 64/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1388 - accuracy: 0.9510 - val_loss: 0.1168 - val_accuracy: 0.9592\n",
      "Epoch 65/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1380 - accuracy: 0.9512 - val_loss: 0.1130 - val_accuracy: 0.9592\n",
      "Epoch 66/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1365 - accuracy: 0.9525 - val_loss: 0.1119 - val_accuracy: 0.9600\n",
      "Epoch 67/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1357 - accuracy: 0.9512 - val_loss: 0.1112 - val_accuracy: 0.9608\n",
      "Epoch 68/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1354 - accuracy: 0.9512 - val_loss: 0.1150 - val_accuracy: 0.9615\n",
      "Epoch 69/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1352 - accuracy: 0.9530 - val_loss: 0.1103 - val_accuracy: 0.9615\n",
      "Epoch 70/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1363 - accuracy: 0.9520 - val_loss: 0.1086 - val_accuracy: 0.9615\n",
      "Epoch 71/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1339 - accuracy: 0.9523 - val_loss: 0.1093 - val_accuracy: 0.9623\n",
      "Epoch 72/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1319 - accuracy: 0.9525 - val_loss: 0.1086 - val_accuracy: 0.9623\n",
      "Epoch 73/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1330 - accuracy: 0.9551 - val_loss: 0.1081 - val_accuracy: 0.9631\n",
      "Epoch 74/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1313 - accuracy: 0.9528 - val_loss: 0.1057 - val_accuracy: 0.9608\n",
      "Epoch 75/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1305 - accuracy: 0.9530 - val_loss: 0.1096 - val_accuracy: 0.9669\n",
      "Epoch 76/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1300 - accuracy: 0.9551 - val_loss: 0.1053 - val_accuracy: 0.9631\n",
      "Epoch 77/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1288 - accuracy: 0.9559 - val_loss: 0.1039 - val_accuracy: 0.9631\n",
      "Epoch 78/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1271 - accuracy: 0.9546 - val_loss: 0.1022 - val_accuracy: 0.9615\n",
      "Epoch 79/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1273 - accuracy: 0.9543 - val_loss: 0.1019 - val_accuracy: 0.9631\n",
      "Epoch 80/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1268 - accuracy: 0.9546 - val_loss: 0.1058 - val_accuracy: 0.9677\n",
      "Epoch 81/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1278 - accuracy: 0.9571 - val_loss: 0.1018 - val_accuracy: 0.9623\n",
      "Epoch 82/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1246 - accuracy: 0.9566 - val_loss: 0.1020 - val_accuracy: 0.9685\n",
      "Epoch 83/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1254 - accuracy: 0.9551 - val_loss: 0.0996 - val_accuracy: 0.9654\n",
      "Epoch 84/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1247 - accuracy: 0.9554 - val_loss: 0.0986 - val_accuracy: 0.9646\n",
      "Epoch 85/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1231 - accuracy: 0.9577 - val_loss: 0.0978 - val_accuracy: 0.9631\n",
      "Epoch 86/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1226 - accuracy: 0.9559 - val_loss: 0.0973 - val_accuracy: 0.9646\n",
      "Epoch 87/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1217 - accuracy: 0.9569 - val_loss: 0.0977 - val_accuracy: 0.9692\n",
      "Epoch 88/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1209 - accuracy: 0.9569 - val_loss: 0.0960 - val_accuracy: 0.9631\n",
      "Epoch 89/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1207 - accuracy: 0.9566 - val_loss: 0.0956 - val_accuracy: 0.9662\n",
      "Epoch 90/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1205 - accuracy: 0.9574 - val_loss: 0.0970 - val_accuracy: 0.9700\n",
      "Epoch 91/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1201 - accuracy: 0.9564 - val_loss: 0.0951 - val_accuracy: 0.9692\n",
      "Epoch 92/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1187 - accuracy: 0.9574 - val_loss: 0.0936 - val_accuracy: 0.9677\n",
      "Epoch 93/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1181 - accuracy: 0.9587 - val_loss: 0.0928 - val_accuracy: 0.9677\n",
      "Epoch 94/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1186 - accuracy: 0.9582 - val_loss: 0.0925 - val_accuracy: 0.9692\n",
      "Epoch 95/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1169 - accuracy: 0.9582 - val_loss: 0.0923 - val_accuracy: 0.9700\n",
      "Epoch 96/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1163 - accuracy: 0.9613 - val_loss: 0.0911 - val_accuracy: 0.9685\n",
      "Epoch 97/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1164 - accuracy: 0.9597 - val_loss: 0.0908 - val_accuracy: 0.9669\n",
      "Epoch 98/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1160 - accuracy: 0.9602 - val_loss: 0.0904 - val_accuracy: 0.9685\n",
      "Epoch 99/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1156 - accuracy: 0.9592 - val_loss: 0.0935 - val_accuracy: 0.9715\n",
      "Epoch 100/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1188 - accuracy: 0.9577 - val_loss: 0.0917 - val_accuracy: 0.9731\n",
      "Epoch 101/2000\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1185 - accuracy: 0.9636 - val_loss: 0.0936 - val_accuracy: 0.9638\n",
      "Epoch 102/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1150 - accuracy: 0.9607 - val_loss: 0.0916 - val_accuracy: 0.9723\n",
      "Epoch 103/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1172 - accuracy: 0.9589 - val_loss: 0.0956 - val_accuracy: 0.9738\n",
      "Epoch 104/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1166 - accuracy: 0.9602 - val_loss: 0.0924 - val_accuracy: 0.9646\n",
      "Epoch 105/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1147 - accuracy: 0.9618 - val_loss: 0.0865 - val_accuracy: 0.9700\n",
      "Epoch 106/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1119 - accuracy: 0.9607 - val_loss: 0.0886 - val_accuracy: 0.9738\n",
      "Epoch 107/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1121 - accuracy: 0.9625 - val_loss: 0.0856 - val_accuracy: 0.9708\n",
      "Epoch 108/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.1110 - accuracy: 0.9651 - val_loss: 0.0855 - val_accuracy: 0.9708\n",
      "Epoch 109/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1111 - accuracy: 0.9600 - val_loss: 0.0858 - val_accuracy: 0.9738\n",
      "Epoch 110/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1093 - accuracy: 0.9628 - val_loss: 0.0841 - val_accuracy: 0.9700\n",
      "Epoch 111/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1089 - accuracy: 0.9630 - val_loss: 0.0856 - val_accuracy: 0.9738\n",
      "Epoch 112/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1089 - accuracy: 0.9628 - val_loss: 0.0841 - val_accuracy: 0.9746\n",
      "Epoch 113/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1079 - accuracy: 0.9651 - val_loss: 0.0828 - val_accuracy: 0.9723\n",
      "Epoch 114/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1076 - accuracy: 0.9643 - val_loss: 0.0829 - val_accuracy: 0.9746\n",
      "Epoch 115/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1075 - accuracy: 0.9638 - val_loss: 0.0826 - val_accuracy: 0.9754\n",
      "Epoch 116/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1067 - accuracy: 0.9633 - val_loss: 0.0817 - val_accuracy: 0.9754\n",
      "Epoch 117/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1060 - accuracy: 0.9664 - val_loss: 0.0810 - val_accuracy: 0.9754\n",
      "Epoch 118/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1065 - accuracy: 0.9636 - val_loss: 0.0818 - val_accuracy: 0.9746\n",
      "Epoch 119/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1059 - accuracy: 0.9659 - val_loss: 0.0801 - val_accuracy: 0.9769\n",
      "Epoch 120/2000\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1052 - accuracy: 0.9646 - val_loss: 0.0803 - val_accuracy: 0.9731\n",
      "Epoch 121/2000\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1046 - accuracy: 0.9651 - val_loss: 0.0808 - val_accuracy: 0.9746\n",
      "Epoch 122/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1045 - accuracy: 0.9661 - val_loss: 0.0794 - val_accuracy: 0.9762\n",
      "Epoch 123/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1037 - accuracy: 0.9666 - val_loss: 0.0800 - val_accuracy: 0.9754\n",
      "Epoch 124/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1027 - accuracy: 0.9669 - val_loss: 0.0788 - val_accuracy: 0.9723\n",
      "Epoch 125/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1041 - accuracy: 0.9656 - val_loss: 0.0786 - val_accuracy: 0.9754\n",
      "Epoch 126/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1024 - accuracy: 0.9666 - val_loss: 0.0781 - val_accuracy: 0.9723\n",
      "Epoch 127/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1027 - accuracy: 0.9636 - val_loss: 0.0807 - val_accuracy: 0.9769\n",
      "Epoch 128/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1031 - accuracy: 0.9651 - val_loss: 0.0787 - val_accuracy: 0.9777\n",
      "Epoch 129/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1020 - accuracy: 0.9669 - val_loss: 0.0774 - val_accuracy: 0.9754\n",
      "Epoch 130/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1020 - accuracy: 0.9672 - val_loss: 0.0813 - val_accuracy: 0.9700\n",
      "Epoch 131/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1037 - accuracy: 0.9666 - val_loss: 0.0761 - val_accuracy: 0.9762\n",
      "Epoch 132/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1000 - accuracy: 0.9659 - val_loss: 0.0761 - val_accuracy: 0.9762\n",
      "Epoch 133/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1004 - accuracy: 0.9674 - val_loss: 0.0755 - val_accuracy: 0.9762\n",
      "Epoch 134/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0993 - accuracy: 0.9695 - val_loss: 0.0744 - val_accuracy: 0.9777\n",
      "Epoch 135/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0990 - accuracy: 0.9679 - val_loss: 0.0744 - val_accuracy: 0.9777\n",
      "Epoch 136/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0984 - accuracy: 0.9682 - val_loss: 0.0763 - val_accuracy: 0.9792\n",
      "Epoch 137/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1007 - accuracy: 0.9661 - val_loss: 0.0737 - val_accuracy: 0.9777\n",
      "Epoch 138/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0994 - accuracy: 0.9674 - val_loss: 0.0755 - val_accuracy: 0.9746\n",
      "Epoch 139/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0983 - accuracy: 0.9713 - val_loss: 0.0740 - val_accuracy: 0.9785\n",
      "Epoch 140/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0965 - accuracy: 0.9697 - val_loss: 0.0725 - val_accuracy: 0.9777\n",
      "Epoch 141/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0967 - accuracy: 0.9705 - val_loss: 0.0728 - val_accuracy: 0.9800\n",
      "Epoch 142/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0968 - accuracy: 0.9674 - val_loss: 0.0744 - val_accuracy: 0.9808\n",
      "Epoch 143/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0963 - accuracy: 0.9718 - val_loss: 0.0717 - val_accuracy: 0.9792\n",
      "Epoch 144/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0954 - accuracy: 0.9702 - val_loss: 0.0713 - val_accuracy: 0.9792\n",
      "Epoch 145/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0954 - accuracy: 0.9697 - val_loss: 0.0714 - val_accuracy: 0.9785\n",
      "Epoch 146/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0950 - accuracy: 0.9702 - val_loss: 0.0709 - val_accuracy: 0.9792\n",
      "Epoch 147/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0944 - accuracy: 0.9720 - val_loss: 0.0709 - val_accuracy: 0.9808\n",
      "Epoch 148/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0946 - accuracy: 0.9700 - val_loss: 0.0718 - val_accuracy: 0.9823\n",
      "Epoch 149/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0951 - accuracy: 0.9702 - val_loss: 0.0704 - val_accuracy: 0.9815\n",
      "Epoch 150/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0948 - accuracy: 0.9705 - val_loss: 0.0736 - val_accuracy: 0.9746\n",
      "Epoch 151/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0972 - accuracy: 0.9697 - val_loss: 0.0715 - val_accuracy: 0.9762\n",
      "Epoch 152/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0976 - accuracy: 0.9672 - val_loss: 0.0696 - val_accuracy: 0.9815\n",
      "Epoch 153/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0932 - accuracy: 0.9679 - val_loss: 0.0708 - val_accuracy: 0.9815\n",
      "Epoch 154/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0928 - accuracy: 0.9702 - val_loss: 0.0686 - val_accuracy: 0.9792\n",
      "Epoch 155/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0935 - accuracy: 0.9725 - val_loss: 0.0685 - val_accuracy: 0.9792\n",
      "Epoch 156/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0917 - accuracy: 0.9720 - val_loss: 0.0678 - val_accuracy: 0.9800\n",
      "Epoch 157/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0912 - accuracy: 0.9715 - val_loss: 0.0675 - val_accuracy: 0.9823\n",
      "Epoch 158/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0916 - accuracy: 0.9705 - val_loss: 0.0687 - val_accuracy: 0.9769\n",
      "Epoch 159/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0911 - accuracy: 0.9731 - val_loss: 0.0676 - val_accuracy: 0.9815\n",
      "Epoch 160/2000\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0924 - accuracy: 0.9690 - val_loss: 0.0732 - val_accuracy: 0.9800\n",
      "Epoch 161/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0915 - accuracy: 0.9684 - val_loss: 0.0670 - val_accuracy: 0.9831\n",
      "Epoch 162/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0904 - accuracy: 0.9713 - val_loss: 0.0663 - val_accuracy: 0.9823\n",
      "Epoch 163/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0893 - accuracy: 0.9728 - val_loss: 0.0670 - val_accuracy: 0.9792\n",
      "Epoch 164/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0910 - accuracy: 0.9718 - val_loss: 0.0654 - val_accuracy: 0.9808\n",
      "Epoch 165/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0897 - accuracy: 0.9692 - val_loss: 0.0709 - val_accuracy: 0.9815\n",
      "Epoch 166/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0904 - accuracy: 0.9715 - val_loss: 0.0657 - val_accuracy: 0.9831\n",
      "Epoch 167/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0889 - accuracy: 0.9728 - val_loss: 0.0670 - val_accuracy: 0.9785\n",
      "Epoch 168/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0913 - accuracy: 0.9749 - val_loss: 0.0668 - val_accuracy: 0.9785\n",
      "Epoch 169/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0913 - accuracy: 0.9677 - val_loss: 0.0682 - val_accuracy: 0.9846\n",
      "Epoch 170/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0903 - accuracy: 0.9700 - val_loss: 0.0706 - val_accuracy: 0.9831\n",
      "Epoch 171/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0896 - accuracy: 0.9705 - val_loss: 0.0653 - val_accuracy: 0.9808\n",
      "Epoch 172/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0887 - accuracy: 0.9731 - val_loss: 0.0660 - val_accuracy: 0.9792\n",
      "Epoch 173/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0893 - accuracy: 0.9720 - val_loss: 0.0655 - val_accuracy: 0.9838\n",
      "Epoch 174/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0882 - accuracy: 0.9705 - val_loss: 0.0683 - val_accuracy: 0.9846\n",
      "Epoch 175/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0883 - accuracy: 0.9720 - val_loss: 0.0629 - val_accuracy: 0.9831\n",
      "Epoch 176/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0874 - accuracy: 0.9743 - val_loss: 0.0628 - val_accuracy: 0.9815\n",
      "Epoch 177/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0855 - accuracy: 0.9743 - val_loss: 0.0638 - val_accuracy: 0.9815\n",
      "Epoch 178/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0865 - accuracy: 0.9718 - val_loss: 0.0623 - val_accuracy: 0.9831\n",
      "Epoch 179/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0859 - accuracy: 0.9723 - val_loss: 0.0636 - val_accuracy: 0.9838\n",
      "Epoch 180/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0845 - accuracy: 0.9710 - val_loss: 0.0632 - val_accuracy: 0.9838\n",
      "Epoch 181/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0856 - accuracy: 0.9736 - val_loss: 0.0619 - val_accuracy: 0.9831\n",
      "Epoch 182/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0848 - accuracy: 0.9743 - val_loss: 0.0619 - val_accuracy: 0.9815\n",
      "Epoch 183/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0838 - accuracy: 0.9731 - val_loss: 0.0695 - val_accuracy: 0.9846\n",
      "Epoch 184/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0883 - accuracy: 0.9736 - val_loss: 0.0652 - val_accuracy: 0.9846\n",
      "Epoch 185/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0868 - accuracy: 0.9725 - val_loss: 0.0611 - val_accuracy: 0.9838\n",
      "Epoch 186/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0865 - accuracy: 0.9728 - val_loss: 0.0676 - val_accuracy: 0.9777\n",
      "Epoch 187/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0875 - accuracy: 0.9710 - val_loss: 0.0611 - val_accuracy: 0.9846\n",
      "Epoch 188/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0842 - accuracy: 0.9738 - val_loss: 0.0611 - val_accuracy: 0.9846\n",
      "Epoch 189/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0820 - accuracy: 0.9738 - val_loss: 0.0627 - val_accuracy: 0.9838\n",
      "Epoch 190/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0829 - accuracy: 0.9741 - val_loss: 0.0610 - val_accuracy: 0.9846\n",
      "Epoch 191/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0821 - accuracy: 0.9743 - val_loss: 0.0611 - val_accuracy: 0.9831\n",
      "Epoch 192/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0826 - accuracy: 0.9741 - val_loss: 0.0594 - val_accuracy: 0.9854\n",
      "Epoch 193/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0813 - accuracy: 0.9733 - val_loss: 0.0631 - val_accuracy: 0.9854\n",
      "Epoch 194/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0830 - accuracy: 0.9749 - val_loss: 0.0597 - val_accuracy: 0.9846\n",
      "Epoch 195/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0832 - accuracy: 0.9761 - val_loss: 0.0592 - val_accuracy: 0.9846\n",
      "Epoch 196/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0812 - accuracy: 0.9751 - val_loss: 0.0592 - val_accuracy: 0.9846\n",
      "Epoch 197/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0813 - accuracy: 0.9746 - val_loss: 0.0595 - val_accuracy: 0.9846\n",
      "Epoch 198/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0803 - accuracy: 0.9769 - val_loss: 0.0587 - val_accuracy: 0.9854\n",
      "Epoch 199/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0802 - accuracy: 0.9759 - val_loss: 0.0599 - val_accuracy: 0.9838\n",
      "Epoch 200/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0804 - accuracy: 0.9741 - val_loss: 0.0610 - val_accuracy: 0.9862\n",
      "Epoch 201/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0809 - accuracy: 0.9749 - val_loss: 0.0599 - val_accuracy: 0.9854\n",
      "Epoch 202/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0800 - accuracy: 0.9756 - val_loss: 0.0593 - val_accuracy: 0.9846\n",
      "Epoch 203/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0793 - accuracy: 0.9756 - val_loss: 0.0584 - val_accuracy: 0.9846\n",
      "Epoch 204/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0787 - accuracy: 0.9766 - val_loss: 0.0587 - val_accuracy: 0.9846\n",
      "Epoch 205/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0785 - accuracy: 0.9756 - val_loss: 0.0578 - val_accuracy: 0.9854\n",
      "Epoch 206/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0778 - accuracy: 0.9759 - val_loss: 0.0589 - val_accuracy: 0.9846\n",
      "Epoch 207/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0786 - accuracy: 0.9751 - val_loss: 0.0591 - val_accuracy: 0.9854\n",
      "Epoch 208/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0795 - accuracy: 0.9756 - val_loss: 0.0573 - val_accuracy: 0.9846\n",
      "Epoch 209/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0771 - accuracy: 0.9761 - val_loss: 0.0584 - val_accuracy: 0.9869\n",
      "Epoch 210/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0793 - accuracy: 0.9743 - val_loss: 0.0581 - val_accuracy: 0.9854\n",
      "Epoch 211/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0790 - accuracy: 0.9756 - val_loss: 0.0595 - val_accuracy: 0.9869\n",
      "Epoch 212/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0792 - accuracy: 0.9746 - val_loss: 0.0608 - val_accuracy: 0.9862\n",
      "Epoch 213/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0801 - accuracy: 0.9743 - val_loss: 0.0578 - val_accuracy: 0.9854\n",
      "Epoch 214/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0798 - accuracy: 0.9746 - val_loss: 0.0559 - val_accuracy: 0.9862\n",
      "Epoch 215/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0768 - accuracy: 0.9769 - val_loss: 0.0558 - val_accuracy: 0.9862\n",
      "Epoch 216/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0765 - accuracy: 0.9769 - val_loss: 0.0555 - val_accuracy: 0.9862\n",
      "Epoch 217/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0779 - accuracy: 0.9759 - val_loss: 0.0569 - val_accuracy: 0.9854\n",
      "Epoch 218/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0756 - accuracy: 0.9759 - val_loss: 0.0554 - val_accuracy: 0.9854\n",
      "Epoch 219/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0756 - accuracy: 0.9766 - val_loss: 0.0555 - val_accuracy: 0.9862\n",
      "Epoch 220/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0756 - accuracy: 0.9756 - val_loss: 0.0550 - val_accuracy: 0.9862\n",
      "Epoch 221/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0753 - accuracy: 0.9777 - val_loss: 0.0554 - val_accuracy: 0.9869\n",
      "Epoch 222/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0768 - accuracy: 0.9754 - val_loss: 0.0554 - val_accuracy: 0.9869\n",
      "Epoch 223/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0754 - accuracy: 0.9766 - val_loss: 0.0558 - val_accuracy: 0.9854\n",
      "Epoch 224/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0751 - accuracy: 0.9769 - val_loss: 0.0562 - val_accuracy: 0.9862\n",
      "Epoch 225/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0747 - accuracy: 0.9774 - val_loss: 0.0543 - val_accuracy: 0.9869\n",
      "Epoch 226/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0743 - accuracy: 0.9769 - val_loss: 0.0542 - val_accuracy: 0.9869\n",
      "Epoch 227/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0741 - accuracy: 0.9774 - val_loss: 0.0545 - val_accuracy: 0.9877\n",
      "Epoch 228/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0739 - accuracy: 0.9761 - val_loss: 0.0565 - val_accuracy: 0.9877\n",
      "Epoch 229/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0744 - accuracy: 0.9759 - val_loss: 0.0540 - val_accuracy: 0.9862\n",
      "Epoch 230/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0732 - accuracy: 0.9772 - val_loss: 0.0536 - val_accuracy: 0.9869\n",
      "Epoch 231/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0730 - accuracy: 0.9774 - val_loss: 0.0537 - val_accuracy: 0.9877\n",
      "Epoch 232/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0730 - accuracy: 0.9787 - val_loss: 0.0536 - val_accuracy: 0.9869\n",
      "Epoch 233/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0730 - accuracy: 0.9772 - val_loss: 0.0534 - val_accuracy: 0.9869\n",
      "Epoch 234/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0731 - accuracy: 0.9777 - val_loss: 0.0579 - val_accuracy: 0.9877\n",
      "Epoch 235/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0747 - accuracy: 0.9759 - val_loss: 0.0547 - val_accuracy: 0.9877\n",
      "Epoch 236/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0738 - accuracy: 0.9764 - val_loss: 0.0571 - val_accuracy: 0.9846\n",
      "Epoch 237/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0779 - accuracy: 0.9756 - val_loss: 0.0585 - val_accuracy: 0.9823\n",
      "Epoch 238/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0781 - accuracy: 0.9769 - val_loss: 0.0579 - val_accuracy: 0.9885\n",
      "Epoch 239/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0735 - accuracy: 0.9772 - val_loss: 0.0526 - val_accuracy: 0.9869\n",
      "Epoch 240/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0733 - accuracy: 0.9766 - val_loss: 0.0526 - val_accuracy: 0.9877\n",
      "Epoch 241/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0745 - accuracy: 0.9754 - val_loss: 0.0543 - val_accuracy: 0.9877\n",
      "Epoch 242/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0718 - accuracy: 0.9779 - val_loss: 0.0524 - val_accuracy: 0.9877\n",
      "Epoch 243/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0712 - accuracy: 0.9792 - val_loss: 0.0534 - val_accuracy: 0.9869\n",
      "Epoch 244/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0711 - accuracy: 0.9769 - val_loss: 0.0535 - val_accuracy: 0.9885\n",
      "Epoch 245/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0709 - accuracy: 0.9787 - val_loss: 0.0520 - val_accuracy: 0.9869\n",
      "Epoch 246/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0708 - accuracy: 0.9772 - val_loss: 0.0528 - val_accuracy: 0.9877\n",
      "Epoch 247/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0716 - accuracy: 0.9766 - val_loss: 0.0517 - val_accuracy: 0.9869\n",
      "Epoch 248/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0732 - accuracy: 0.9777 - val_loss: 0.0516 - val_accuracy: 0.9885\n",
      "Epoch 249/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0727 - accuracy: 0.9774 - val_loss: 0.0547 - val_accuracy: 0.9892\n",
      "Epoch 250/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0725 - accuracy: 0.9769 - val_loss: 0.0557 - val_accuracy: 0.9885\n",
      "Epoch 251/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0712 - accuracy: 0.9769 - val_loss: 0.0520 - val_accuracy: 0.9869\n",
      "Epoch 252/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0709 - accuracy: 0.9772 - val_loss: 0.0526 - val_accuracy: 0.9885\n",
      "Epoch 253/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0702 - accuracy: 0.9777 - val_loss: 0.0517 - val_accuracy: 0.9869\n",
      "Epoch 254/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0703 - accuracy: 0.9795 - val_loss: 0.0524 - val_accuracy: 0.9877\n",
      "Epoch 255/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0704 - accuracy: 0.9774 - val_loss: 0.0510 - val_accuracy: 0.9869\n",
      "Epoch 256/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0693 - accuracy: 0.9784 - val_loss: 0.0509 - val_accuracy: 0.9869\n",
      "Epoch 257/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0701 - accuracy: 0.9777 - val_loss: 0.0513 - val_accuracy: 0.9877\n",
      "Epoch 258/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0689 - accuracy: 0.9784 - val_loss: 0.0510 - val_accuracy: 0.9885\n",
      "Epoch 259/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0704 - accuracy: 0.9766 - val_loss: 0.0514 - val_accuracy: 0.9869\n",
      "Epoch 260/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0712 - accuracy: 0.9764 - val_loss: 0.0546 - val_accuracy: 0.9877\n",
      "Epoch 261/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0714 - accuracy: 0.9787 - val_loss: 0.0505 - val_accuracy: 0.9885\n",
      "Epoch 262/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0695 - accuracy: 0.9784 - val_loss: 0.0504 - val_accuracy: 0.9885\n",
      "Epoch 263/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0694 - accuracy: 0.9764 - val_loss: 0.0502 - val_accuracy: 0.9877\n",
      "Epoch 264/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0688 - accuracy: 0.9790 - val_loss: 0.0543 - val_accuracy: 0.9892\n",
      "Epoch 265/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0690 - accuracy: 0.9782 - val_loss: 0.0505 - val_accuracy: 0.9877\n",
      "Epoch 266/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0678 - accuracy: 0.9790 - val_loss: 0.0501 - val_accuracy: 0.9877\n",
      "Epoch 267/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0680 - accuracy: 0.9795 - val_loss: 0.0505 - val_accuracy: 0.9892\n",
      "Epoch 268/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0688 - accuracy: 0.9790 - val_loss: 0.0514 - val_accuracy: 0.9892\n",
      "Epoch 269/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0683 - accuracy: 0.9787 - val_loss: 0.0549 - val_accuracy: 0.9885\n",
      "Epoch 270/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0691 - accuracy: 0.9790 - val_loss: 0.0497 - val_accuracy: 0.9885\n",
      "Epoch 271/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0674 - accuracy: 0.9790 - val_loss: 0.0495 - val_accuracy: 0.9885\n",
      "Epoch 272/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0670 - accuracy: 0.9792 - val_loss: 0.0532 - val_accuracy: 0.9892\n",
      "Epoch 273/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0687 - accuracy: 0.9784 - val_loss: 0.0499 - val_accuracy: 0.9877\n",
      "Epoch 274/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0689 - accuracy: 0.9782 - val_loss: 0.0500 - val_accuracy: 0.9877\n",
      "Epoch 275/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0673 - accuracy: 0.9795 - val_loss: 0.0493 - val_accuracy: 0.9877\n",
      "Epoch 276/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0674 - accuracy: 0.9795 - val_loss: 0.0509 - val_accuracy: 0.9900\n",
      "Epoch 277/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0684 - accuracy: 0.9779 - val_loss: 0.0492 - val_accuracy: 0.9885\n",
      "Epoch 278/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0687 - accuracy: 0.9800 - val_loss: 0.0497 - val_accuracy: 0.9885\n",
      "Epoch 279/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0672 - accuracy: 0.9787 - val_loss: 0.0489 - val_accuracy: 0.9877\n",
      "Epoch 280/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0663 - accuracy: 0.9795 - val_loss: 0.0489 - val_accuracy: 0.9877\n",
      "Epoch 281/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0662 - accuracy: 0.9805 - val_loss: 0.0488 - val_accuracy: 0.9877\n",
      "Epoch 282/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0658 - accuracy: 0.9800 - val_loss: 0.0488 - val_accuracy: 0.9885\n",
      "Epoch 283/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0662 - accuracy: 0.9797 - val_loss: 0.0485 - val_accuracy: 0.9885\n",
      "Epoch 284/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0665 - accuracy: 0.9790 - val_loss: 0.0511 - val_accuracy: 0.9892\n",
      "Epoch 285/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0665 - accuracy: 0.9808 - val_loss: 0.0499 - val_accuracy: 0.9908\n",
      "Epoch 286/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0669 - accuracy: 0.9795 - val_loss: 0.0492 - val_accuracy: 0.9885\n",
      "Epoch 287/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0653 - accuracy: 0.9797 - val_loss: 0.0508 - val_accuracy: 0.9900\n",
      "Epoch 288/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0663 - accuracy: 0.9802 - val_loss: 0.0488 - val_accuracy: 0.9908\n",
      "Epoch 289/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0650 - accuracy: 0.9800 - val_loss: 0.0487 - val_accuracy: 0.9908\n",
      "Epoch 290/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0648 - accuracy: 0.9805 - val_loss: 0.0483 - val_accuracy: 0.9892\n",
      "Epoch 291/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0646 - accuracy: 0.9813 - val_loss: 0.0480 - val_accuracy: 0.9885\n",
      "Epoch 292/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0646 - accuracy: 0.9802 - val_loss: 0.0481 - val_accuracy: 0.9908\n",
      "Epoch 293/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0644 - accuracy: 0.9808 - val_loss: 0.0489 - val_accuracy: 0.9915\n",
      "Epoch 294/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0644 - accuracy: 0.9813 - val_loss: 0.0480 - val_accuracy: 0.9877\n",
      "Epoch 295/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0643 - accuracy: 0.9802 - val_loss: 0.0488 - val_accuracy: 0.9915\n",
      "Epoch 296/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0647 - accuracy: 0.9810 - val_loss: 0.0478 - val_accuracy: 0.9885\n",
      "Epoch 297/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0644 - accuracy: 0.9815 - val_loss: 0.0480 - val_accuracy: 0.9877\n",
      "Epoch 298/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0648 - accuracy: 0.9810 - val_loss: 0.0480 - val_accuracy: 0.9877\n",
      "Epoch 299/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0672 - accuracy: 0.9797 - val_loss: 0.0481 - val_accuracy: 0.9915\n",
      "Epoch 300/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0647 - accuracy: 0.9795 - val_loss: 0.0546 - val_accuracy: 0.9877\n",
      "Epoch 301/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0688 - accuracy: 0.9790 - val_loss: 0.0491 - val_accuracy: 0.9900\n",
      "Epoch 302/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0696 - accuracy: 0.9774 - val_loss: 0.0475 - val_accuracy: 0.9892\n",
      "Epoch 303/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0671 - accuracy: 0.9797 - val_loss: 0.0479 - val_accuracy: 0.9885\n",
      "Epoch 304/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0639 - accuracy: 0.9802 - val_loss: 0.0472 - val_accuracy: 0.9892\n",
      "Epoch 305/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0642 - accuracy: 0.9808 - val_loss: 0.0475 - val_accuracy: 0.9908\n",
      "Epoch 306/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0636 - accuracy: 0.9813 - val_loss: 0.0494 - val_accuracy: 0.9892\n",
      "Epoch 307/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0637 - accuracy: 0.9808 - val_loss: 0.0471 - val_accuracy: 0.9892\n",
      "Epoch 308/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0630 - accuracy: 0.9823 - val_loss: 0.0469 - val_accuracy: 0.9900\n",
      "Epoch 309/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0629 - accuracy: 0.9815 - val_loss: 0.0472 - val_accuracy: 0.9908\n",
      "Epoch 310/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0635 - accuracy: 0.9815 - val_loss: 0.0469 - val_accuracy: 0.9908\n",
      "Epoch 311/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0625 - accuracy: 0.9813 - val_loss: 0.0469 - val_accuracy: 0.9908\n",
      "Epoch 312/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0626 - accuracy: 0.9808 - val_loss: 0.0468 - val_accuracy: 0.9900\n",
      "Epoch 313/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0627 - accuracy: 0.9808 - val_loss: 0.0469 - val_accuracy: 0.9892\n",
      "Epoch 314/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0633 - accuracy: 0.9805 - val_loss: 0.0479 - val_accuracy: 0.9908\n",
      "Epoch 315/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0640 - accuracy: 0.9815 - val_loss: 0.0470 - val_accuracy: 0.9915\n",
      "Epoch 316/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0629 - accuracy: 0.9810 - val_loss: 0.0490 - val_accuracy: 0.9900\n",
      "Epoch 317/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0624 - accuracy: 0.9823 - val_loss: 0.0466 - val_accuracy: 0.9885\n",
      "Epoch 318/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0632 - accuracy: 0.9813 - val_loss: 0.0497 - val_accuracy: 0.9892\n",
      "Epoch 319/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0662 - accuracy: 0.9805 - val_loss: 0.0504 - val_accuracy: 0.9877\n",
      "Epoch 320/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0644 - accuracy: 0.9787 - val_loss: 0.0464 - val_accuracy: 0.9908\n",
      "Epoch 321/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0635 - accuracy: 0.9805 - val_loss: 0.0528 - val_accuracy: 0.9877\n",
      "Epoch 322/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0652 - accuracy: 0.9802 - val_loss: 0.0496 - val_accuracy: 0.9892\n",
      "Epoch 323/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0645 - accuracy: 0.9797 - val_loss: 0.0464 - val_accuracy: 0.9923\n",
      "Epoch 324/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0626 - accuracy: 0.9818 - val_loss: 0.0462 - val_accuracy: 0.9900\n",
      "Epoch 325/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0621 - accuracy: 0.9826 - val_loss: 0.0481 - val_accuracy: 0.9892\n",
      "Epoch 326/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0647 - accuracy: 0.9800 - val_loss: 0.0475 - val_accuracy: 0.9900\n",
      "Epoch 327/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0637 - accuracy: 0.9818 - val_loss: 0.0468 - val_accuracy: 0.9900\n",
      "Epoch 328/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0639 - accuracy: 0.9792 - val_loss: 0.0476 - val_accuracy: 0.9915\n",
      "Epoch 329/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0629 - accuracy: 0.9808 - val_loss: 0.0464 - val_accuracy: 0.9923\n",
      "Epoch 330/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0627 - accuracy: 0.9805 - val_loss: 0.0467 - val_accuracy: 0.9908\n",
      "Epoch 331/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0611 - accuracy: 0.9820 - val_loss: 0.0460 - val_accuracy: 0.9900\n",
      "Epoch 332/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0610 - accuracy: 0.9823 - val_loss: 0.0461 - val_accuracy: 0.9923\n",
      "Epoch 333/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0605 - accuracy: 0.9818 - val_loss: 0.0464 - val_accuracy: 0.9915\n",
      "Epoch 334/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0623 - accuracy: 0.9815 - val_loss: 0.0472 - val_accuracy: 0.9915\n",
      "Epoch 335/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0606 - accuracy: 0.9826 - val_loss: 0.0457 - val_accuracy: 0.9900\n",
      "Epoch 336/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0604 - accuracy: 0.9833 - val_loss: 0.0458 - val_accuracy: 0.9923\n",
      "Epoch 337/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0604 - accuracy: 0.9826 - val_loss: 0.0469 - val_accuracy: 0.9892\n",
      "Epoch 338/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0621 - accuracy: 0.9823 - val_loss: 0.0463 - val_accuracy: 0.9892\n",
      "Epoch 339/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0625 - accuracy: 0.9802 - val_loss: 0.0489 - val_accuracy: 0.9900\n",
      "Epoch 340/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0639 - accuracy: 0.9815 - val_loss: 0.0463 - val_accuracy: 0.9892\n",
      "Epoch 341/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0613 - accuracy: 0.9818 - val_loss: 0.0457 - val_accuracy: 0.9915\n",
      "Epoch 342/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0612 - accuracy: 0.9792 - val_loss: 0.0463 - val_accuracy: 0.9908\n",
      "Epoch 343/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0609 - accuracy: 0.9823 - val_loss: 0.0455 - val_accuracy: 0.9923\n",
      "Epoch 344/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0595 - accuracy: 0.9826 - val_loss: 0.0459 - val_accuracy: 0.9900\n",
      "Epoch 345/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0593 - accuracy: 0.9828 - val_loss: 0.0476 - val_accuracy: 0.9908\n",
      "Epoch 346/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0597 - accuracy: 0.9836 - val_loss: 0.0457 - val_accuracy: 0.9908\n",
      "Epoch 347/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0603 - accuracy: 0.9826 - val_loss: 0.0456 - val_accuracy: 0.9900\n",
      "Epoch 348/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0607 - accuracy: 0.9823 - val_loss: 0.0457 - val_accuracy: 0.9900\n",
      "Epoch 349/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0587 - accuracy: 0.9831 - val_loss: 0.0455 - val_accuracy: 0.9915\n",
      "Epoch 350/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0599 - accuracy: 0.9828 - val_loss: 0.0454 - val_accuracy: 0.9900\n",
      "Epoch 351/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0614 - accuracy: 0.9800 - val_loss: 0.0453 - val_accuracy: 0.9900\n",
      "Epoch 352/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0610 - accuracy: 0.9805 - val_loss: 0.0456 - val_accuracy: 0.9915\n",
      "Epoch 353/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0599 - accuracy: 0.9810 - val_loss: 0.0473 - val_accuracy: 0.9908\n",
      "Epoch 354/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0607 - accuracy: 0.9818 - val_loss: 0.0511 - val_accuracy: 0.9869\n",
      "Epoch 355/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0597 - accuracy: 0.9815 - val_loss: 0.0455 - val_accuracy: 0.9915\n",
      "Epoch 356/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0588 - accuracy: 0.9815 - val_loss: 0.0505 - val_accuracy: 0.9892\n",
      "Epoch 357/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0597 - accuracy: 0.9823 - val_loss: 0.0459 - val_accuracy: 0.9900\n",
      "Epoch 358/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0615 - accuracy: 0.9841 - val_loss: 0.0463 - val_accuracy: 0.9900\n",
      "Epoch 359/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0610 - accuracy: 0.9797 - val_loss: 0.0446 - val_accuracy: 0.9908\n",
      "Epoch 360/2000\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0584 - accuracy: 0.9841 - val_loss: 0.0452 - val_accuracy: 0.9900\n",
      "Epoch 361/2000\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0588 - accuracy: 0.9831 - val_loss: 0.0453 - val_accuracy: 0.9908\n",
      "Epoch 362/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0583 - accuracy: 0.9826 - val_loss: 0.0469 - val_accuracy: 0.9908\n",
      "Epoch 363/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0584 - accuracy: 0.9826 - val_loss: 0.0482 - val_accuracy: 0.9892\n",
      "Epoch 364/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0590 - accuracy: 0.9820 - val_loss: 0.0449 - val_accuracy: 0.9915\n",
      "Epoch 365/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0583 - accuracy: 0.9831 - val_loss: 0.0477 - val_accuracy: 0.9908\n",
      "Epoch 366/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0583 - accuracy: 0.9831 - val_loss: 0.0451 - val_accuracy: 0.9900\n",
      "Epoch 367/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0596 - accuracy: 0.9805 - val_loss: 0.0455 - val_accuracy: 0.9908\n",
      "Epoch 368/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0574 - accuracy: 0.9833 - val_loss: 0.0511 - val_accuracy: 0.9885\n",
      "Epoch 369/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0609 - accuracy: 0.9805 - val_loss: 0.0458 - val_accuracy: 0.9892\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, y_train, epochs= 2000, batch_size= 500, validation_split=0.25, callbacks=[early_stopping,checkpointer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 2ms/step - loss: 0.0565 - accuracy: 0.9792\n",
      "Test accuracy :  0.9792307615280151\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy : ', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
